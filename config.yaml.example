# ── Matrix (recommended) ─────────────────────────────────────────────
# Remove or leave empty to disable Matrix. The web UI works standalone.
matrix:
  homeserver: https://matrix.org          # Your Matrix homeserver URL
  user_id: "@bot:matrix.org"              # Full Matrix user ID of the bot account

  # Option A (recommended): supply password — Wintermute logs in automatically,
  # fills in access_token/device_id below, and refreshes them on expiry.
  password: ""
  # Option B: supply token + device_id from a manual login (leave password empty).
  access_token: ""
  device_id: ""                           # Auto-filled when using password login.

  allowed_users:                          # Matrix user IDs allowed to interact
    - "@admin:matrix.org"
  allowed_rooms: []                       # Room ID whitelist (empty = any room)

# ── Web Interface ─────────────────────────────────────────────────
# Enabled by default. Open http://127.0.0.1:8080 in your browser.
web:
  enabled: true
  host: "127.0.0.1"                       # Use 0.0.0.0 to listen on all interfaces
  port: 8080

# ── Inference Backends ─────────────────────────────────────────────
# Named backend definitions. Each describes one endpoint+model combination.
# Referenced by name in the llm: role mapping.
inference_backends:
  - name: "local_large"
    provider: "openai"                    # "openai", "gemini-cli", or "kimi-code"
    base_url: "http://localhost:8080/v1"
    api_key: "llama-server"
    model: "qwen2.5:72b"
    context_size: 32768
    max_tokens: 4096
    reasoning: false

  - name: "local_small"
    provider: "openai"
    base_url: "http://localhost:8080/v1"
    api_key: "llama-server"
    model: "qwen2.5:7b"
    context_size: 32768
    max_tokens: 2048
    reasoning: false

  # UNSTABLE/ALPHA — gemini-cli piggybacks on Google's Cloud Code Assist
  # OAuth flow.  Credentials may expire unpredictably.  Not recommended
  # as your only backend.
  # - name: "gemini_pro"
  #   provider: "gemini-cli"
  #   model: "gemini-2.5-pro"
  #   context_size: 1048576
  #   max_tokens: 8192

  # Kimi-Code — flat-rate subscription with device-code OAuth.
  # Auth: uv run python -m wintermute.kimi_auth  (or /kimi-auth in chat)
  # Credentials: data/kimi_credentials.json
  # - name: "kimi"
  #   provider: "kimi-code"
  #   model: "kimi-for-coding"
  #   context_size: 131072
  #   max_tokens: 8192

  # - name: "turing_backend"
  #   provider: "openai"
  #   base_url: "http://localhost:8080/v1"
  #   api_key: "llama-server"
  #   model: "qwen2.5:7b"
  #   context_size: 32768
  #   max_tokens: 150

# ── LLM Role Mapping ──────────────────────────────────────────────
# Maps roles to ordered lists of backend names.
# Multiple backends = automatic failover on API errors.
# Empty list [] = disabled.
# Omitted roles default to the first defined backend.
llm:
  base: ["local_large"]
  compaction: ["local_small", "local_large"] # strongly recommended to use the same llm, or one with the same context size as the main model
  sub_sessions: ["local_large"]
  dreaming: ["local_small"]

# -- Turing Protocol (Post-Inference Validation) ---------------------
turing_protocol:
  backends: ["local_small"]               # small/fast model recommended
  validators:
    workflow_spawn: true                   # detect hallucinated workflow spawn claims
    phantom_tool_result: true              # detect fabricated tool output claims
    empty_promise: true                    # detect unfulfilled action commitments

# ── Context Compaction ────────────────────────────────────────────
# Compaction fires when history tokens exceed:
#   context_size - max_tokens - system_prompt_tokens
# No separate threshold setting is needed; adjust in the backend definition.

# ── Pulse Reviews ─────────────────────────────────────────────────
pulse:
  enabled: true                           # Set to false to disable pulse reviews
  review_interval_minutes: 60             # How often to auto-review pulse items

# ── Component Size Limits ─────────────────────────────────────────
# Characters before a component triggers AI auto-summarisation.
context:
  component_size_limits:
    memories: 10000                       # MEMORIES.txt
    pulse: 5000                           # Pulse items (DB)
    skills_total: 20000                   # Total across all skills/*.md

# ── Dreaming (Nightly Consolidation) ─────────────────────────────
dreaming:
  hour: 1                                 # Hour (0-23, UTC) to run consolidation
  minute: 0                               # Minute (0-59)

# ── Scheduler ─────────────────────────────────────────────────────
scheduler:
  timezone: "UTC"                         # Timezone for reminder scheduling (e.g. Europe/Berlin)

# ── Logging ───────────────────────────────────────────────────────
logging:
  level: "INFO"                           # DEBUG | INFO | WARN | ERROR
  directory: "logs"
