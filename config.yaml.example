# Matrix is optional. Remove or leave empty to disable.
matrix:
  homeserver: https://matrix.org          # Your Matrix homeserver URL
  user_id: "@bot:matrix.org"              # Full Matrix user ID of the bot account

  # Option A (recommended): supply password — Wintermute logs in automatically,
  # fills in access_token/device_id below, and refreshes them on expiry.
  password: ""
  # Option B: supply token + device_id from a manual login (leave password empty).
  access_token: ""
  device_id: ""                           # Auto-filled when using password login.
                                          # The bot cross-signs this device automatically at startup.

  allowed_users:                          # Matrix user IDs allowed to interact
    - "@admin:matrix.org"
  allowed_rooms: []                       # Room ID whitelist (empty = any room if invited by allowed user)

# Web interface is enabled by default.
# Open http://127.0.0.1:8080 in your browser.
web:
  enabled: true
  host: "127.0.0.1"   # Use 0.0.0.0 to listen on all interfaces
  port: 8080

# ── Inference Backends ─────────────────────────────────────────────
# Named LLM backend definitions.  Each entry describes one endpoint+model
# combination.  Backends are referenced by name in the llm: role mapping below.
inference_backends:
  - name: "local_large"
    provider: "openai"                    # "openai" (any OpenAI-compatible) or "gemini-cli"
    base_url: "http://localhost:11434/v1" # Ollama / vLLM / LM Studio / OpenAI / etc.
    api_key: "ollama"                     # Use "ollama" for Ollama, actual key for OpenAI etc.
    model: "qwen2.5:72b"
    context_size: 32768                   # Total token window the model supports
    max_tokens: 4096                      # Maximum tokens in a single response
    reasoning: false                      # Enable for o1/o3, DeepSeek R1, QwQ, etc.

  - name: "local_small"
    provider: "openai"
    base_url: "http://localhost:11434/v1"
    api_key: "ollama"
    model: "qwen2.5:7b"
    context_size: 32768
    max_tokens: 2048
    reasoning: false

  # Example: Google Gemini via gemini-cli (free, requires `npm i -g @google/gemini-cli`).
  # Run `uv run python -m wintermute.gemini_auth` to authenticate.
  # - name: "gemini_pro"
  #   provider: "gemini-cli"
  #   model: "gemini-2.5-pro"
  #   context_size: 1048576
  #   max_tokens: 8192
  #   reasoning: false

  # Example: supervisor-specific backend with small max_tokens.
  # - name: "supervisor_backend"
  #   provider: "openai"
  #   base_url: "http://localhost:11434/v1"
  #   api_key: "ollama"
  #   model: "qwen2.5:7b"
  #   context_size: 32768
  #   max_tokens: 150                     # Supervisor only needs ~150 tokens for JSON response
  #   reasoning: false

# ── LLM Role Mapping ──────────────────────────────────────────────
# Maps each role to an ordered list of backend names (defined above).
# Multiple backends = automatic failover: if the first errors, the next is tried.
# An empty list [] disables the role (only meaningful for supervisor).
# Omitted roles default to the first backend defined above.
llm:
  base: ["local_large"]
  compaction: ["local_small", "local_large"]
  sub_sessions: ["local_large"]
  dreaming: ["local_small"]
  supervisor: ["local_small"]             # Use [] to disable supervisor checks

# Compaction fires when history tokens exceed: context_size - max_tokens - system_prompt_tokens.
# No separate threshold setting is needed; adjust context_size and max_tokens in the backend.

pulse:
  enabled: true                           # Set to false to disable periodic pulse reviews
  review_interval_minutes: 60             # How often to auto-review PULSE.txt

context:
  component_size_limits:
    memories: 10000                       # Chars before MEMORIES.txt gets summarised
    pulse: 5000                      # Chars before pulse.txt gets summarised
    skills_total: 20000                   # Total chars across all skills before reorganise

dreaming:
  hour: 1                                 # Hour (0-23) to run nightly memory consolidation
  minute: 0                               # Minute (0-59)

scheduler:
  timezone: "UTC"                         # Timezone for reminder scheduling (e.g. Europe/Berlin)

logging:
  level: "INFO"                           # DEBUG | INFO | WARN | ERROR
  directory: "logs"
