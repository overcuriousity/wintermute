# Matrix is optional. Remove or leave empty to disable.
matrix:
  homeserver: https://matrix.org          # Your Matrix homeserver URL
  user_id: "@bot:matrix.org"              # Full Matrix user ID of the bot account

  # Option A (recommended): supply password â€” Wintermute logs in automatically,
  # fills in access_token/device_id below, and refreshes them on expiry.
  password: ""
  # Option B: supply token + device_id from a manual login (leave password empty).
  access_token: ""
  device_id: ""                           # Auto-filled when using password login.
                                          # The bot cross-signs this device automatically at startup.

  allowed_users:                          # Matrix user IDs allowed to interact
    - "@admin:matrix.org"
  allowed_rooms: []                       # Room ID whitelist (empty = any room if invited by allowed user)

# Web interface is enabled by default.
# Open http://127.0.0.1:8080 in your browser.
web:
  enabled: true
  host: "127.0.0.1"   # Use 0.0.0.0 to listen on all interfaces
  port: 8080

llm:
  # Any OpenAI-compatible endpoint. Examples:
  #   Ollama local:   http://localhost:11434/v1
  #   vLLM:          http://localhost:8000/v1
  #   LM Studio:     http://localhost:1234/v1
  #   OpenAI:        https://api.openai.com/v1
  base_url: "http://localhost:11434/v1"
  api_key: "ollama"                        # Use "ollama" for Ollama, actual key for OpenAI etc.
  model: "qwen2.5:72b"                     # Any model name the endpoint accepts
  context_size: 32768                      # Total token window the model supports
  max_tokens: 4096                         # Maximum tokens in a single response
  # Enable for reasoning/thinking models (OpenAI o1/o3, DeepSeek R1, QwQ, etc.).
  # Uses max_completion_tokens instead of max_tokens and extracts reasoning tokens.
  # Reasoning is shown in the web UI but hidden from Matrix to reduce noise.
  # reasoning: true

  # Optional per-purpose overrides. Unspecified fields inherit from the parent llm: block.
  # Each sub-key can override: base_url, api_key, model, context_size, max_tokens, reasoning.
  compaction:
    model: "qwen2.5:7b"                     # Cheaper/faster model for context compaction
    # base_url: "https://api.openai.com/v1" # Use a different provider entirely
    # api_key: "sk-..."
    # context_size: 128000
    # max_tokens: 2048
    # reasoning: false
  # sub_sessions:                            # Override for background sub-session workers
  #   base_url: "https://api.openai.com/v1"
  #   api_key: "sk-..."
  #   model: "gpt-4o"
  #   context_size: 128000
  #   max_tokens: 8192
  #   reasoning: false
  # dreaming:                                # Override for nightly memory consolidation
  #   base_url: "https://api.openai.com/v1"
  #   api_key: "sk-..."
  #   model: "qwen2.5:7b"
  #   context_size: 32768
  #   max_tokens: 2048
  #   reasoning: false

# Compaction fires when history tokens exceed: context_size - max_tokens - system_prompt_tokens.
# No separate threshold setting is needed; adjust context_size and max_tokens above.

pulse:
  review_interval_minutes: 60             # How often to auto-review PULSE.txt

context:
  component_size_limits:
    memories: 10000                       # Chars before MEMORIES.txt gets summarised
    pulse: 5000                      # Chars before pulse.txt gets summarised
    skills_total: 20000                   # Total chars across all skills before reorganise

dreaming:
  hour: 1                                 # Hour (0-23) to run nightly memory consolidation
  minute: 0                               # Minute (0-59)

scheduler:
  timezone: "UTC"                         # Timezone for reminder scheduling (e.g. Europe/Berlin)

logging:
  level: "INFO"                           # DEBUG | INFO | WARN | ERROR
  directory: "logs"
